\section*{Discussion}

We present an entirely new approach to performing and communicating
experimental science, here demonstrated for physiology. We propose
that three \emph{types} (signals, events and durations), when
parametrised by any other type, are sufficient to represent most or
all evidence for this field. We show how observations and calculations
of these types can be described in a mathematical framework based on
the lambda calculus. We use two experiments from neurophysiology to
demonstrate that this approach works in practice: the \emph{in vivo}
spike train response to a visual looming stimulus in locusts; and an
analysis of synaptic integration with dynamic clamp in zebrafish. Our use
of typed, functional and reactive programming overcomes at least three
long-standing issues in bioinformatics: the need for a flexible
ontology to share heterogeneous data from physiological experiments
\cite{Amari2002}, a language for describing experiments unambiguously
\cite{Murray-Rust2002}, and an equational formulation of data
provenance \cite{Pool2002}. In addition, the tradition of
\emph{formal verification} attached to the lambda calculus
\cite{Harrison2009,DeBruijn1968,Bird1996,Hindley2008}
suggests the possibility that algorithmic procedures can complement
peer-review in validating scientific inference.

\subsection*{An ontology for physiology}

There are clear advantages to sharing primary data from scientific
experiments \cite{Insel2003} which include: preventing needless replication,
facilitating meta-analysis, giving theoretical neuroscientists access
to a greater variety of observations, and enhancing transparency. These
benefits are likely to be greater if data are structured and stored in
standard formats. Despite these advantages, and many attempted
databases, few data from electrophysiological experiments are
shared in practise. This is likely due to both technical and social
barriers \cite{Amari2002}. We suggest that modifying the approach to
experimentation may help overcome social barriers by completely
integrating the experiment and analysis specification with structured
data storage that requires little annotation.

The types we have presented form a linguistic framework and an
ontology for physiology that address the technical barriers to data
sharing. Thanks to the flexibility of parametric polymorphism, our
ontology can form the basis for the interchange of physiological data
without imposing unnecessary constraints on what can be shared. The
ontology is non-hierarchical and would be difficult to formulate in the
various existing semantic web ontology frameworks (Web Ontology
Language or Resource Description Framework), which lack parametric
polymorphism and functional abstraction. Nevertheless, in specifying
the categories of mathematical objects that constitute evidence, it is
an ontology in the classical sense of cataloguing the categories within
a specific domain, and providing a vocabulary for that domain. We
emphasise again that it is an ontology of \emph{evidence}, not of the
biological entities that give rise to this evidence. It is unusual as
an ontology for scientific knowledge in being embedded in a
\emph{computational} framework, such that it can describe not only
mathematical objects but also their transformations and observations.

Existing software packages used for the acquisition and analysis of
physiological data have signal-like (Igor Pro, Wavemetrics; Spike 2,
Cambridge Electronic Design), and event-like (Spike 2) data structures
at the core of their operations. Although these package have some
flexibility in the information that can be carried by signals and
events, they do not exploit full parametric polymorphism: the range of
observations and stimuli that can be described with their more limited
definitions is smaller than in our framework. This is the case even if
``multi-dimensional'' signals are permitted; this allows a value to
be \emph{indexed} by a vector, but the values themselves remain
scalar. For instance, the signal of shapes that describes a visual
stimulus in Example 1 \emph{cannot} be represented by a signal in
these systems. The two- or three dimensional location of a moving
animal must be represented either by two or three separate signals, or
one must resort to \emph{ad-hoc} tricks, such as using the first
dimension of a multidimensional signal to represent time, and the
remaining dimensions to index the vector dimensions. But this only
works for vector quantities, and forces one to think in terms of linear
algebra. Full parametric polymorphism, on the other hand, gives the
possibility of creating a small vocabulary of generic functions for
data analysis that can transform any time-varying quantity. Therefore,
our framework can be seen as a generalisation of existing methods of
physiological signal processing.

Previous work on scientific knowledge representation has suggested
that ``meta-data'' can represent the context of an experiment
\cite{Bower2009}. This raises at least two questions: what
information must be communicated, and how should it be represented?
Minimal standards for reporting information from scientific studies
have been suggested for different fields
\cite{Taylor2007,Gibson2008}, but remain informal. We suggest
distinguishing the experimental context into those aspects that can
practically be executed by a machine, and those that must be carried
out by a human experimenter. Machine-executable aspects of the
experiment can be described unambiguously, but it is difficult to prove
that human-executable aspects can be similarly described. Even if a
description works well in practice, there may be unstated shared assumptions
between the communicating experimenters. Informal but evolving reporting
standards are therefore likely to be the only way to delimit this information.

In terms of representing experimental contexts, here we need make no
distinction between data and meta-data. All relevant information which
can be represented by values in some type can exist as a collection of
signals, events and durations and be stored in the same location as
the primary observations. There are important reasons to believe that
there cannot exist a strict distinction between data and
meta-data. Information that seems incidental and contextual to the
primary experimenter may after dissemination become important in
subsequent studies. As an example, we point to the measurements of
glutamate spillover in the hippocampus, where the temperature at which
early experiments \cite{Kullmann1996} were conducted, when manipulated
in a later study \cite{Asztely1997}, influenced key
findings.  In our framework, we do not impose any such distinction on
information. The different values are all indexed by time and thus
linked by overlap on a common time scale.

\subsection*{Experiment descriptions}

Using the calculus of physiological evidence, both stimuli and
observations are defined concisely and unambiguously by mathematical
\emph{equations}. This makes it possible to repeat, manipulate and
reason about experiments in a formal framework. Our mathematical
definitions are less ambiguous than definitions written in plain
English, and are more powerful than those specified by graphical user
interfaces or in formal languages that lack a facility for defining
abstractions. Our approach also makes the conditions of the recording
very explicit, and can serve as an unambiguous medium for automating
and communicating experiments. 

Our framework is not only a theoretical formalism, but also a very
practical tool. It is a collection of computer programs for executing
experiments and analyses, and carrying out data
management. Controlling experimentation and analysis with programs
that share data formats can be highly efficient and eliminates
many sources of human error. Our experiment definitions have the
further advantage that they \emph{compose}; that is, more complex
experiments can be formulated by joining together elementary building
blocks. 

The use of a formal language for formulating experiments does not preclude the
use of a graphical user interface for helping to construct these formulations.
On the contrary, user-friendly interfaces can be built to generate
code in language such as CoPE. This allows designers of innovative user
interfaces for experimentation and analysis to focus on that aspect, while
benefiting from an expressive, formally defined foundation for
realising new functionality, facilitating interoperability, 
% HN 2010-11-10: This sentence just didn't parse.
% not having to worry about
and interacting with hardware in a timely manner.
%  as this is already taken care of.
Graphical user interfaces can also be an efficient way of
entering specific values based on the observed data. For instance, we have
used a simple interface to enter the thresholds for spike detection in Example
1. We plan to facilitate the description of such \emph{ad-hoc} interfaces and
to build more elaborate interfaces as front-ends to explore observations.

\input{gensig}

\subsection*{Statistics}

We have used the word ``evidence'' to mean direct observations and
calculated values from experiments. Evidence thus carries information
that is relevant for statistical models of the systems under study,
but we have not yet extended our approach to include statistical
analyses. How could values with signal, event or duration types be
incorporated into statistical models? A conservative approach would be to
take measurements on signals and events -- for instance the amplitudes
of signal deflections, or the frequencies of event -- and store these in
durations. The tags of durations representing measurements could then
be used in classical null-hypothesis significance
tests such as the General Linear Model. 

A more intriguing possibility is to build statistical models for the
directly observed data \cite{Daniell1991}, and to use nested
durations to describe a hierarchical organisation \cite{Rouder2003}
of conditional dependencies amongst model parameters. In the context
of physiology, this could be achieved by augmenting a BUGS-like
\cite{Gilks1994} language with constructors and distributions for
signals, events and durations.


\subsection*{Towards verified scientific inference}

If we consider that science is based on logic \cite{Jaynes2003}, it
must be possible in principle to mechanically verify scientific
inference, just as mathematical proofs can be verified by a proof
checker \cite{Harrison2009}. It is of course not possible to verify
particular hypotheses about the physical world, or an organism. What
might be verifiable are statements about experiments --- for instance,
that: particular variables were randomly controlled and not observed;
outcomes have not gained correlation due to the analysis procedure;
missing data are accounted for by the statistical model
\cite{Gelman2003}; errors are propagated correctly \cite{Taylor1997};
units of measurement are used consistently \cite{Kennedy1997}; there
is no ``double dipping'' \cite{Kriegeskorte2009}; and, ultimately,
that the observed data support the conclusions drawn. Statistical
analyses address some of these issues in relating observations to
parameter estimation and hypothesis testing. However, without knowing
where observations come from, it is difficult to ascertain whether
they provide evidence for a given scientific hypothesis
\cite{Pool2002}. Experiment description languages, and the
representation of experimental observations into values of concrete
types (which may not always be real numbers), could play an important
role in such inference. The statistical framework within which such
inferences take place has an impact on the amount of information that
must be analysed. For instance, if we accept the likelihood principle
\cite{Jaynes2003}, we can safely ignore the intention of the
experimenter, because all the relevant information is in the
likelihood of the observed data.

There has been substantial progress in \emph{automation} in the
experimental sciences \cite{King2004}. In contrast, there has been
almost no work in algorithmic \emph{verification} \cite{Kropf1999,
  Sadot}, which is a separate but overlapping application of
computers to science. Nevertheless, if such verification is
feasible it may lead to a radical change in the way scientific
research is conducted and communicated. It is likely that at least
some aspects of validation in physiology can be achieved with conservative
extensions of
% the calculus of physiological evidence 
CoPE integrated with statistical inference.

\section*{Methods}

\subsection*{Language implementation} 

We have used two different implementation strategies for reasons of
rapid development and execution efficiency. For purposes of
experimentation and simulation, we have implemented a prototype
compiler that can execute some programs that contain signals and
events defined by mutual recursion, as is necessary for many of the
simulations and experiments in this paper. The program is transformed
by the compiler into a normal form that is translated to an imperative
program which iteratively updates variables corresponding to signal
values, with a time step that is set explicitly. The program is
divided into a series of stages, where each stage consists of the
signals and events defined by mutual recursion, subject to the
constraints of input/output sources and sinks. This ensures that
signal expressions can reference values of other signals at arbitrary
time points (possibly in the future) as long as referenced signals are
computed in an earlier stage.

To calculate a new value from existing observations after data
acquisition, we have implemented the calculus of physiological
evidence as a domain-specific language embedded in the purely functional
programming language Haskell. 

For hard real-time dynamic-clamp experiments, we have built a compiler
back-end targeting the LXRT (user-space) interface to the RTAI (Real-time
application interface; http://rtai.org) extensions of the Linux
kernel, and the Comedi (http://comedi.org) interface to data
acquisition hardware. Geometric shapes were rendered using OpenGL
(http://opengl.org).

All code is available at http://github.com/glutamate/bugpan and
released under the GPL.

\subsection*{Locust experiments}

Recordings from locust DCMD neurons were performed as previously
described \cite{Matheson2004}. Briefly, locusts were fixed in
plasticine with the ventral side upwards. The head was fixed with wax
% at a $90^{\circ}$ angle 
and the connectives were exposed through an
incision in the soft tissue of the neck. A pair of silver wire hook
electrodes were placed underneath the connectives and the electrodes
and connectives enclosed in petroleum jelly. The electrode signal was
amplified 1000x and bandpass filtered 50--5000 Hz, before
analog-to-digital conversion at 18 bits and 20 kHz with a National
Instruments PCI-6281 board. The locust was placed in front of a 22''
CRT monitor running with a vertical refresh rate of 160 Hz. All
aspects of the visual stimulus displayed on this monitor and of
the analog-to-digital conversion performed by the National Instruments
board were controlled by programs written in 
% the calculus of physiological evidence
CoPE running on a single computer.

\subsection*{Zebrafish experiments}

Intracellular patch-clamp recordings from motor neurons in the spinal
cord from a 2-day old zebrafish embryo were performed as previously
described \cite{McDearmid2006}. We used a National Instruments PCI-6281
board to
record the output from a BioLogic patch-clamp amplifier in
current-clamp mode, filtered at 3kHz and digitised at 10 kHz, with the
output current calculated at the same rate by programs written in
% the calculus of physiological evidence 
CoPE targeted to the RTAI backend (see
above). The measured jitter for updating the output voltage was 6
$\mu$s and was similar to that measured with the RTAI latency test
tool for the experiment control computer.

\section*{Acknowledgements} 

We would like to thank Jonathan McDearmid for help with the Zebrafish
recordings and Angus Silver, Guy Billings, Antonia Hamilton, Nick
Hartell and Rodrigo Quian Quiroga for critical comments on the
manuscript. This work was funded by the Human Frontier Science Project
fellowship to TAN, a Biotechnology and Biological Sciences Research
Council grant to TM and TAN, and Engineering and Physical Sciences Research
Council grants to HN.

\section*{Author Contributions}  
T.N. designed and implemented CoPE, carried out the experiments and
data analyses, and wrote the draft of the paper. H.N. contributed to
the language design, helped clarify the semantics, and wrote several
sections of the manuscript. T.M. contributed to the design of the
experiments and the data analysis, and made extensive comments on
drafts of the manuscript. All authors obtained grant funding to
support this project as described in the acknowledgements.


