\section*{Discussion}

We present an entirely new approach to performing and communicating
science. We propose that three \emph{types}, signals, events and
durations, are sufficient to represent physiological evidence when
they can be parametrised by any other type. We show how observations
and calculations of these types can be described in a mathematical
framework based on the lambda calculus. We use two experiments from
neurophysiology to demonstrate that this approach works in practise:
the \emph{in vivo} spike train response to a visual looming stimulus
in locusts; and a study of synaptic integration in with dynamic clamp.

Using our approach, both stimuli and observations are defined
concisely and unambiguously by mathematical \emph{equations}. This
makes it possible to repeat, manipulate and reason about experiments
in a formal framework. Our mathematical definitions are less ambiguous
than definitions written in plain English, and are more powerful than
those specified by graphical user interfaces or in formal languages
without a facility for defining abstractions.

To our knowledge this is the first explicit use of simple type theory
to classify evidence in a experimental scientific field. We find that
parametric polymorphism is critical in encoding a wide repertoire of
evidence. We also surprisingly found that there is broad overlap
between the types that hold stimuli, observations and calculated
values, although this may be not be the case in other fields, for
instance in anatomy. A wider question is which type theories can
support evidence from many different fields. For physiology, we have
shown that simple type theory \citep{Pierce2002} can support key types
of evidence. Richer type theories including impredicativity and
dependent types \citep{Pierce2002} may help capture aspects of
evidence in other fields.

We have presented a mathematical approach to experimentation at three
levels. 1. We present it is a very practical tool: a collection of
computer programs for executing experiments and analyses, and data
management. Controlling every aspect of the scientific process with
programs that share data formats is potentially highly efficient and
eliminates many sources of human error. 2. Beyond any particular
implementation, we have described a framework for reasoning about and
manipulating scientific experiments. Obtaining such an experiment
definition is a necessary step in automating an experiment.  It also
makes the conditions of the recording very explicit, and can serve as
an unambiguous communication medium. 3. The types we have
presented form a linguistic framework and an ontology for
physiology. This ontology can form the basis for the interchange of
physiological data without imposing unnecessary constraints on what
can be shared.

We have claimed that most or all physiological evidence can be
represented by signals, events and durations. We have shown two very
different examples from neurophysiology in this paper to substantiate
our claim. In addition, we have used the calculus of physiological
evidence to analyse an experiment with movement data and simultaneous
recording of many neurons from the locust, and analysed the
relationship between neural activity in identified cells and bursts in
other neurons or movement cycles (Calas \& Matheson, in
preparation). These analyses used exactly the same functions and types
as those described above. However, our framework could be
extended. Frequency-domain information is important in analysing
oscillations in neuronal systems. Power-spectra can be thought of as
values analogous to signals, but indexed by frequency rather than
time. The most appropriate manner of extending our framework to
include frequency-indexed and even spatial information must remain a
topic for further research. On the other hand, the lack of linear
algebra, higher order statistics, and information theoretic analysis
in our examples is \emph{not} to be taken as a limitation of our
theory; representing such analyses in a mathematical framework such as
ours is a neither a difficult conceptual problem nor the topic of this
paper.

\subsection*{Statistics}

We have used the word ``evidence'' to mean direct observations and
calculated values from experiments. In another sense of the word,
evidence carries information that is relevant for a particular
statistical model. How can values with signal, event or duration types
be combined with statistical models? First, a conservative approach is
to take measurements on signals and events -- for instance the
amplitude of signal deflections, event frequencies etc -- and store
these in durations. The language of event detectors and list semantics
for durations and events may be sufficiently rich to describe these
measurements, as we have demonstrated for the limited examples in this
paper. It is then straightforward to use the tags of durations
representing measurements in classical null-hypothesis significance
tests such as the General Linear Model. This approach differs little
from the way data are analysed routinely in physiology, but a
lightweight and executable formalism for describing these analyses
would make it easier to handle large amounts of data and reduce the
risk of human error.

A more intriguing possibility is to build statistical models for the
directly observed data, and to use nested durations to describe a
hierarchical organisation of conditional dependencies amongst
parameters in such a model. Although probabilistic models for direct
observations are used in physics \citep{Daniell1991}, the flexible
construction of such models for physiological observations may depend
on the development of new programming languages.

\subsection*{Relation to existing technologies}

Existing software packages used for the acquisition and analysis of
physiological data have signal-like (Igor Pro, Wavemetrics; Spike 2,
Cambridge Electronic Design), and event-like (Spike 2) data structures
at the core of their operations. Although these package have some
flexibility in the information that can be carried by signals and
events, they do not exploit full parametric polymorphism: the range of
observations and stimuli that can be described with their more limited
definitions is smaller than in our framework. For instance, the signal
of shapes that describes a visual stimulus in Example 1, or the two-
or three dimensional location of a moving animal \emph{cannot} be
represented by a signal in these systems. Although these data may be
respresentable in different data structures, they cannot then be
manipulated by functions written to transform generic signals.

In particular, we suggest that two oft-used technologies in knowledge
representation, relational databases and the semantic web, are not
optimally tuned to representing physiological data (a task for which
they were not designed).

Our account of physiological evidence solves or immediately proposes
solutions to many problems in neuroinformatics. One such problem is
the difficulty of sharing primary data \citep{Amari2002}. We have
shown that fully polymorphic signals, events and durations can
represent data across many aspects of neurophysiology, which could
provide the foundation for a very general database. Previous work on
scientific knowledge representation has suggested a possible role for
``meta-data'' to represent the context of an experiment
\citep{Bower2009}. Here, we make no distinction between data and
meta-data. All relevant information which can be represented by values
in some type can exist as a collection of signals, events and
durations. There are important reasons to believe that there cannot
exist a strict distinction between data and meta-data. Is, for
instance, a measurement of the temperature at which an experiment is
conducted, data or meta-data?  That surely depends on the intention of
the experimenter and whether temperature is manipulated, but also
depends on the person who is analysing or meta-analysing the data. In
our framework, we do not impose any limits on the nature of such
information. That does not imply that individual pieces of information
have no context. Because every value has a temporal context, the
entire context of that value can be retrieved by gathering other
values with a similar or enclosing temporal context.

There has been some work on formalising ``workflows'' and creating
``workflow engines'' for perform large batches of standardised data
analyses. In the contrasting approach here, all information about the
experiment is defined by mathematical equations and exists as values
of well-defined types, and analyses are simply mathematical functions
that transform and combine signals, event and durations.

We do not exclude the possibility that an elaborate user interface
could help formulate these equations, or construct new values in
ad-hoc circumstances. Indeed, we have used a simple interface to enter
the thresholds for spike detection in Example 1. Such interfaces,
however, may be unnecessary for scientific inference, even in the
large scale. This is especially likely to be the case when we can
write generic functions over types with very flexible definitions.

Lastly, there has been much work on semantic web ontologies that
describe results from biological experiments. While our approach is at
variance with some of this work, in other respects it is 
complementary. There is much relevant information about the
experiments performed here that we have not represented; for instance,
the exact position of the locust with respect to the screen, the anatomical
location of the recording electrodes etc. Such information should be
represented as a duration, but the question is what type the tag of
the duration should have. One solution is to allow tags to take values
that correspond to subjects or relations in a semantic web ontology.

\subsection*{Towards verified scientific inference}

If we consider that science is based on logic \citep{Jaynes2003}, it
must be possible in principle to mechanically verify scientific
inference, just as mathematical proofs can be verified by a proof
checker \citep{Harrison2009}. It is of course not possible to verify particular
hypotheses about the physical world, or an organism. What can be
verified are statements about experiments --- for instance, that:
particular variables were randomly controlled and not observed;
outcomes have not gained correlation due to the analysis procedure;
missing data is accounted for by the statistical model
\citep{Gelman2003}; errors are propagated correctly
\citep{Taylor1997}; units of measure are used consistently
\citep{Kennedy1997}; there is no ``double dipping''
\citep{Kriegeskorte2009}; and ultimately, that the gathered data
support the conclusions drawn. Statistics addresses some of these
issues in relating observations to parameter estimation and hypothesis
testing, but every procedure makes assumptions about how those
observations are obtained. Experiment description languages, and the
reification of experimental observations into values of concrete types
(which may not always be the real numbers), can play an
important role in such inference. The statistical framework within
which such inferences take place has an impact on the amount of
information that must be analysed. For instance, if we accept the
likelihood principle \citep{Jaynes2003}, we can safely ignore the
intention of the experimenter, because all the relevant information is
in the likelihood of the observed data. 

There has been substantial progress in \emph{automation} in the
experimental sciences, which in turn has accelerated the acquisition
of knowledge. In contrast, there has been almost no work in
verification, which is a seperate but overlapping application of
calculating machines to science. Nevertheless, if such verification is
possible it may lead to a much more radical change in the way
scientific research is conducted.

\section*{Methods}

\subsection*{Language implementation} 

We have used two different implementation strategies for reasons of
rapid development and execution efficiency. For purposes of
experimentation and simulation, we have implemented a prototype
compiler that can execute some programs that contain signals and
events defined by mutual recursion, as is necessary for many of the
simulations and experiments in this paper. The program is transformed
into a normal form that can easily be translated to an imperative
program that iteratively updates variable corresponsing to signal
values, with a timestep that is defined by the source or sink sampling
rates if available, or can be set explicitly. The program is divided
into a series of stages, where each stage consistes of the signals and
events defined by mutual recursion, subject to the constraints of
input/output sources and sinks. This ensures that signal expressions
can reference values of other signals at arbitrary time points
(possibly in the future) as long as referenced signals are computed in
an earlier stage.

For post-acquisition/simulation analysis, where one often merely
wishes to calculate a new value from existing observations, we have
implemented the calculus of physiological evidence as domain-specific
language embedded in the purely functional programming language
Haskell.

For hard real-time dynamic-clamp experiments, we build a compiler
backend targeting the LXRT interface to the RTAI extensions of the
Linux kernel, and the Comedi interface to data acquisition
hardware. Geometric shapes were rendered using OpenGL.

All code is available at http://github.com/glutamate/bugpan.

\subsection*{Locust experiments}

Recordings from the locust DCMD neurons were performed as previously
described (ref). Briefly, locusts were fixed in plasticine with the
ventral side upwards. The head was fixed with wax at an 90 degree
angle and the connectives were exposed. A pair of hook electrodes were
placed underneath the connectives and the electrodes and connectives
enclosed in petroleum jelly. The electrode signal was amplified 1000x
and bandpass filtered 50-5000 Hz, before analog-to-digital conversion
at 18 bits and 20 kHz with a National Instruments PCI-6281 board. The
locust was placed in front of a 22'' CRT monitor running with a
vertical refresh rate of 160 Hz. All aspects of the visual stimulus
and analog-to-digital conversion were controlled by a single computer.

\subsection*{Zebrafish experiments}

Intracellular patch-clamp recordings from motor neurons in the spinal
cord from a 2-day old zebrafish embryo were performed as previously
described (ref). We used a National Instruments PCI-6281 board to
record the output from BioLogic patch-clamp amplifier in current-clamp
mode, filtered at 3kHz and digitised at 10 kHz, with the output
current calculated at the same rate. The measured jitter for updating
the output voltage was 6 $\mu$s and was similar to that measured
with the RTAI latency test tool.  

\bibliographystyle{apalike}
\bibliography{paper}

