\section*{Discussion}

We present an entirely new approach to performing and communicating
science. We propose that three \emph{types}, signals, events and
durations, are sufficient to represent physiological evidence when
they can be parametrised by any other type. We show how observations
and calculations of these types can be described in a mathematical
framework based on the lambda calculus. We use two experiments from
neurophysiology to demonstrate that this approach works in practise:
the \emph{in vivo} spike train response to a visual looming stimulus
in locusts; and a study of synaptic integration with dynamic
clamp. Our use of typed, functional and reactive programming overcomes
at least two long-standing issues in informatics; the need for a
flexible ontology to share heterogeneous data from physiological
experiments \citep{Amari2002}, and a language for describing
experiments unambiguously \citep{MurrayRust2002}. In addition, the
tradition of \emph{formal verification} attached to the lambda
calculus suggests the possibility that algorithmic procedures can
complement peer-review in validating scientific inference.

Do we need this paragraph somewhere: We have claimed that most or all
physiological evidence can be represented by signals, events and
durations. We have shown two very different examples from
neurophysiology in this paper to substantiate our claim. In addition,
we have used the calculus of physiological evidence to analyse an
experiment with movement data and simultaneous recording of many
neurons from the locust, and analysed the relationship between neural
activity in identified cells and bursts in other neurons or movement
cycles (Calas \& Matheson, in preparation). These analyses used
exactly the same functions and types as those described above.

\subsection*{Limitations and extensions}

The main omission from our framework for physiological observations
and experiments are spatial (imaging-based) data. Indeed, this is
quite an oversight given the increasing use of microscopic calcium
imaging (refs) and other interactions between physiology and anatomy
refs). Here, we have focussed on a classical physiology because it
presents a more limited scope and because FRP exclusively deals with
temporal and not spatial contexts. Nevertheless, our framework can be
extended to include spatial observations, based on previous work
\citep{Elliott2003}. This extension is based on analogies between
signals and images (which are ``indexed'' by continuous variables,
differing only in the dimensionality of their index), between events
and points, and between durations and regions of space. Points and
regions would have tags for carrying information We suggest retaining
full parametric polymorphism such that images, points and regions can
represent not only colour or intensity, but values in any type. This
would also allow nesting of spatial and temporal contexts such that
movies (signals of images of, say, colour) or time-varying quantities
recorded at a particular location \citep[for instance spot calcium
  measurements;][]{DiGregorio1999} to be representable. These spatial
types and their transformations can also define visual stimuli,
i.e. replace the arbitrary and limited geometric primitives in Example
1.

An additional omission is frequency-domain information, which is
important in analysing neural oscillations (ref). Time-domain signals
transformed into the frequency-domain with e.g. Fourier or Laplace
transforms can be seen as a generalised signal that is indexed by
frequency rather than by time. We plan to combine frequency-domain and
spatial generalisations of signals, events and durations in the future.

We have not pursued any particularly sophisticated mathematical
analyses of our data. Embedding linear algebra or probability theory
in typed, purely functional programming languages is discussed
elsewhere \citep{Eaton2006, Park2005}.

\subsection*{An ontology for physiology}

There are clear advantages to sharing primary data from scientific
experiments \citep{Insel2003}: preventing needless replication,
facilitating meta-analysis, giving theoretical neuroscientists access
to a greater variety of observations and enhancing transparency. These
benefits are likely to be greater if data are structured and stored in
standard formats. Despite these advantages, and many attempted
databases, little data from electrophysiological experiments are
shared in practice. This is likely due to both technical and social
barriers \citep{Amari2002}. We suggest the the approach to
experimentation can overcome social barriers by completely integrating
the experiment and analysis specification with structured data storage
that requires no annotation. 

The types we have presented form a linguistic framework and an
ontology for physiology that address the technical barriers to data
sharing. Due to the flexibility of parametric polymorphism, our
ontology can form the basis for the interchange of physiological data
without imposing unnecessary constraints on what can be shared. The
ontology cannot be formulated in the various semantic web ontology
frameworks (OWL, RDF) which lack parametric polymorphism and
functional abstraction. Nevertheless, it really is an ontology,
specifying the categories of mathematical objects that constitute
evidence. It is unusual as an ontology for scientific knowledge in
being embedded in a \emph{computational} framework, such that it can
describe not only mathematical objects but also their transformations
and observations.

Existing software packages used for the acquisition and analysis of
physiological data have signal-like (Igor Pro, Wavemetrics; Spike 2,
Cambridge Electronic Design), and event-like (Spike 2) data structures
at the core of their operations. Although these package have some
flexibility in the information that can be carried by signals and
events, they do not exploit full parametric polymorphism: the range of
observations and stimuli that can be described with their more limited
definitions is smaller than in our framework. For instance, the signal
of shapes that describes a visual stimulus in Example 1, or the two-
or three dimensional location of a moving animal \emph{cannot} be
represented by a signal in these systems. Although these data may be
respresentable in different data structures, they cannot then be
manipulated by functions written to transform generic
signals. Therefore, our framework can be seen as a generalisation of
existing methods of physiological signal processing.

Our account of physiological evidence solves or immediately proposes
solutions to many problems in neuroinformatics. One such problem is
the difficulty of sharing primary data \citep{Amari2002}. We have
shown that fully polymorphic signals, events and durations can
represent data across many aspects of neurophysiology, which could
provide the foundation for a very general database. Previous work on
scientific knowledge representation has suggested a possible role for
``meta-data'' to represent the context of an experiment
\citep{Bower2009}. Here, we make no distinction between data and
meta-data. All relevant information which can be represented by values
in some type can exist as a collection of signals, events and
durations. There are important reasons to believe that there cannot
exist a strict distinction between data and meta-data. Is, for
instance, a measurement of the temperature at which an experiment is
conducted, data or meta-data?  That surely depends on the intention of
the experimenter and whether temperature is manipulated, but also
depends on the person who is analysing or meta-analysing the data. In
our framework, we do not impose any limits on the nature of such
information. That does not imply that individual pieces of information
have no context. Because every value has a temporal context, the
entire context of that value can be retrieved by gathering other
values with a similar or enclosing temporal context.

There is much relevant information about the experiments performed
here that we find difficult in representing; for instance, the
procedure for preparing the biological specimin. This information is
not manipulated during the experiment and is not needed for a computer
to execute the experiment, but may be of interest for
meta-analysis. We suggest that a more traditional semantic web
ontology may be more suited for representing this information.

\subsection*{Experiment descriptions}

Using our approach, both stimuli and observations are defined
concisely and unambiguously by mathematical \emph{equations}. This
makes it possible to repeat, manipulate and reason about experiments
in a formal framework. Our mathematical definitions are less ambiguous
than definitions written in plain English, and are more powerful than
those specified by graphical user interfaces or in formal languages
without a facility for defining abstractions. It also makes the
conditions of the recording very explicit, and can serve as an
unambiguous communication medium.

Our framework is not only a theoretical formalism, bug also a very
practical tool: a collection of computer programs for executing
experiments and analyses, and data management. Controlling every
aspect of the scientific process with programs that share data formats
is potentially highly efficient and eliminates many sources of human
error. Our experiment definitions have the further advantage that they
\emph{compose}, that is, more complex experiments can be formulated by
joining together elementary building blocks. An explicit
machine-executable definition is also a necessary step in automating
experiments.

There has been some work on formalising ``workflows'' and creating
``workflow engines'' for perform large batches of standardised data
analyses. In the contrasting approach here, all information about the
experiment is defined by mathematical equations and exists as values
of well-defined types, and analyses are simply mathematical functions
that transform and combine signals, event and durations.

We do not exclude the possibility that an elaborate user interface
could help formulate these equations, or construct new values in
ad-hoc circumstances. Indeed, we have used a simple interface to enter
the thresholds for spike detection in Example 1. Such interfaces,
however, may be unnecessary for scientific inference, even in the
large scale.

\subsection*{Statistics}

We have used the word ``evidence'' to mean direct observations and
calculated values from experiments. In another sense of the word,
evidence carries information that is relevant for a particular
statistical model. How can values with signal, event or duration types
be combined with statistical models? First, a conservative approach is
to take measurements on signals and events -- for instance the
amplitude of signal deflections, event frequencies etc -- and store
these in durations. The language of event detectors and list semantics
for durations and events may be sufficiently rich to describe these
measurements, as we have demonstrated for the limited examples in this
paper. It is then straightforward to use the tags of durations
representing measurements in classical null-hypothesis significance
tests such as the General Linear Model. This approach differs little
from the way data are analysed routinely in physiology, but a
lightweight and executable formalism for describing these analyses
would make it easier to handle large amounts of data and reduce the
risk of human error.

A more intriguing possibility is to build statistical models for the
directly observed data, and to use nested durations to describe a
hierarchical organisation of conditional dependencies amongst
parameters in such a model. Although probabilistic models for direct
observations are used in physics \citep{Daniell1991}, the flexible
construction of such models for physiological observations may depend
on the development of new programming languages.

\subsection*{Towards verified scientific inference}

If we consider that science is based on logic \citep{Jaynes2003}, it
must be possible in principle to mechanically verify scientific
inference, just as mathematical proofs can be verified by a proof
checker \citep{Harrison2009}. It is of course not possible to verify particular
hypotheses about the physical world, or an organism. What can be
verified are statements about experiments --- for instance, that:
particular variables were randomly controlled and not observed;
outcomes have not gained correlation due to the analysis procedure;
missing data is accounted for by the statistical model
\citep{Gelman2003}; errors are propagated correctly
\citep{Taylor1997}; units of measure are used consistently
\citep{Kennedy1997}; there is no ``double dipping''
\citep{Kriegeskorte2009}; and ultimately, that the gathered data
support the conclusions drawn. Statistics addresses some of these
issues in relating observations to parameter estimation and hypothesis
testing, but every procedure makes assumptions about how those
observations are obtained. Experiment description languages, and the
reification of experimental observations into values of concrete types
(which may not always be the real numbers), can play an
important role in such inference. The statistical framework within
which such inferences take place has an impact on the amount of
information that must be analysed. For instance, if we accept the
likelihood principle \citep{Jaynes2003}, we can safely ignore the
intention of the experimenter, because all the relevant information is
in the likelihood of the observed data. 

There has been substantial progress in \emph{automation} in the
experimental sciences, which in turn has accelerated the acquisition
of knowledge. In contrast, there has been almost no work in
verification, which is a seperate but overlapping application of
calculating machines to science. Nevertheless, if such verification is
possible it may lead to a much more radical change in the way
scientific research is conducted.

\section*{Methods}

\subsection*{Language implementation} 

We have used two different implementation strategies for reasons of
rapid development and execution efficiency. For purposes of
experimentation and simulation, we have implemented a prototype
compiler that can execute some programs that contain signals and
events defined by mutual recursion, as is necessary for many of the
simulations and experiments in this paper. The program is transformed
into a normal form that can easily be translated to an imperative
program that iteratively updates variable corresponsing to signal
values, with a timestep that is defined by the source or sink sampling
rates if available, or can be set explicitly. The program is divided
into a series of stages, where each stage consistes of the signals and
events defined by mutual recursion, subject to the constraints of
input/output sources and sinks. This ensures that signal expressions
can reference values of other signals at arbitrary time points
(possibly in the future) as long as referenced signals are computed in
an earlier stage.

For post-acquisition/simulation analysis, where one often merely
wishes to calculate a new value from existing observations, we have
implemented the calculus of physiological evidence as domain-specific
language embedded in the purely functional programming language
Haskell.

For hard real-time dynamic-clamp experiments, we build a compiler
backend targeting the LXRT interface to the RTAI extensions of the
Linux kernel, and the Comedi interface to data acquisition
hardware. Geometric shapes were rendered using OpenGL.

All code is available at http://github.com/glutamate/bugpan.

\subsection*{Locust experiments}

Recordings from the locust DCMD neurons were performed as previously
described (ref). Briefly, locusts were fixed in plasticine with the
ventral side upwards. The head was fixed with wax at an 90 degree
angle and the connectives were exposed. A pair of hook electrodes were
placed underneath the connectives and the electrodes and connectives
enclosed in petroleum jelly. The electrode signal was amplified 1000x
and bandpass filtered 50-5000 Hz, before analog-to-digital conversion
at 18 bits and 20 kHz with a National Instruments PCI-6281 board. The
locust was placed in front of a 22'' CRT monitor running with a
vertical refresh rate of 160 Hz. All aspects of the visual stimulus
and analog-to-digital conversion were controlled by a single computer.

\subsection*{Zebrafish experiments}

Intracellular patch-clamp recordings from motor neurons in the spinal
cord from a 2-day old zebrafish embryo were performed as previously
described (ref). We used a National Instruments PCI-6281 board to
record the output from BioLogic patch-clamp amplifier in current-clamp
mode, filtered at 3kHz and digitised at 10 kHz, with the output
current calculated at the same rate. The measured jitter for updating
the output voltage was 6 $\mu$s and was similar to that measured
with the RTAI latency test tool.  

\bibliographystyle{apalike}
\bibliography{paper}

